{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "papermill": {
      "duration": 2177.718488,
      "end_time": "2020-09-11T16:02:55.111435",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-09-11T15:26:37.392947",
      "version": "2.1.0"
    },
    "colab": {
      "name": "ViT_UNet_TF_TPU.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.01304,
          "end_time": "2020-09-11T15:26:41.959908",
          "exception": false,
          "start_time": "2020-09-11T15:26:41.946868",
          "status": "completed"
        },
        "tags": [],
        "id": "Cg9LU-4h0fHd"
      },
      "source": [
        "# Tensorflow Experiments Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvFIPR8Npqwc",
        "outputId": "0ac96534-f834-4b63-d075-d77371fc6813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%script bash\n",
        "\n",
        "git clone https://benayas1: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@github.com/benayas1/vit-unet.git >> /dev/null\n",
        "(cd /content/vit-unet/ && python setup.py bdist_wheel && pip install dist/vit_unet-0.0.1-py3-none-any.whl) >> /dev/null\n",
        "\n",
        "pip install fire >> /dev/null\n",
        "pip install wandb >> /dev/null\n",
        "pip install benatools >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'vit-unet'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:26:41.995909Z",
          "iopub.status.busy": "2020-09-11T15:26:41.995127Z",
          "iopub.status.idle": "2020-09-11T15:27:04.983364Z",
          "shell.execute_reply": "2020-09-11T15:27:04.982583Z"
        },
        "id": "tI1dM5_TnFCG",
        "papermill": {
          "duration": 23.012195,
          "end_time": "2020-09-11T15:27:04.983488",
          "exception": false,
          "start_time": "2020-09-11T15:26:41.971293",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# install benatools library\n",
        "import vit_unet.tf.model as models\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import albumentations\n",
        "import numpy as np\n",
        "import cv2\n",
        "import wandb\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import matplotlib.pyplot as plt\n",
        "from benatools.torch.fitter import ImageFitter\n",
        "import random\n",
        "import os\n",
        "import time as time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from benatools.tf.tpu import (get_device_strategy, init_tpu)\n",
        "from benatools.utils.tools import MultiStratifiedKFold\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# CONSTANTS\n",
        "PLATFORM = 'COLAB'  # this could be 'COLAB' or 'LOCAL'\n",
        "DEVICE = 'TPU'   # This could be 'GPU' or 'CPU'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtjegLSX0fHn"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhfkH4zcYy-J",
        "papermill": {
          "duration": 0.01142,
          "end_time": "2020-09-11T15:27:05.007150",
          "exception": false,
          "start_time": "2020-09-11T15:27:04.995730",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Seeding everything for experiment replicability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:05.036809Z",
          "iopub.status.busy": "2020-09-11T15:27:05.035935Z",
          "iopub.status.idle": "2020-09-11T15:27:05.038842Z",
          "shell.execute_reply": "2020-09-11T15:27:05.038202Z"
        },
        "id": "Uz6eDyk4nQ0z",
        "papermill": {
          "duration": 0.019834,
          "end_time": "2020-09-11T15:27:05.038937",
          "exception": false,
          "start_time": "2020-09-11T15:27:05.019103",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Seed\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "seed_everything(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bimv-klYwEh",
        "papermill": {
          "duration": 0.011595,
          "end_time": "2020-09-11T15:27:05.062538",
          "exception": false,
          "start_time": "2020-09-11T15:27:05.050943",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Setting up distributed strategy. In case of training with TPU's or multiple GPU's, a distributed strategy must be created. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:05.182608Z",
          "iopub.status.busy": "2020-09-11T15:27:05.101411Z",
          "iopub.status.idle": "2020-09-11T15:27:10.039000Z",
          "shell.execute_reply": "2020-09-11T15:27:10.038440Z"
        },
        "id": "8H0wCAbEo1UZ",
        "papermill": {
          "duration": 4.96508,
          "end_time": "2020-09-11T15:27:10.039112",
          "exception": false,
          "start_time": "2020-09-11T15:27:05.074032",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86f5bb4f-ebb5-424a-e7f0-a33b260db2a2"
      },
      "source": [
        "strategy, AUTO, REPLICAS, tpu = get_device_strategy(DEVICE, verbose=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "connecting to TPU...\n",
            "Running on TPU grpc://10.71.20.250:8470\n",
            "initializing TPU ...\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.20.250:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.20.250:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU initialized\n",
            "REPLICAS: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOd-aL1EmNcd",
        "papermill": {
          "duration": 0.012503,
          "end_time": "2020-09-11T15:27:10.131456",
          "exception": false,
          "start_time": "2020-09-11T15:27:10.118953",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxXWUyVSnyN5",
        "papermill": {
          "duration": 0.012948,
          "end_time": "2020-09-11T15:27:10.158651",
          "exception": false,
          "start_time": "2020-09-11T15:27:10.145703",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Dataset Folder\n",
        "If training on TPU, the data must be stored into a GS bucket.  \n",
        "When training on Kaggle platform, calling \n",
        "```python\n",
        "KaggleDatasets().get_gcs_path() \n",
        "```\n",
        "\n",
        "\n",
        "automatically copies the dataset into a GS bucket.  \n",
        "If training on Google Colab, be aware that you might incurr in egress charges.  \n",
        "If training on GPU, there is no such problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:10.204122Z",
          "iopub.status.busy": "2020-09-11T15:27:10.191955Z",
          "iopub.status.idle": "2020-09-11T15:27:13.381441Z",
          "shell.execute_reply": "2020-09-11T15:27:13.381916Z"
        },
        "id": "Q98vpflUmQM8",
        "papermill": {
          "duration": 3.210706,
          "end_time": "2020-09-11T15:27:13.382046",
          "exception": false,
          "start_time": "2020-09-11T15:27:10.171340",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "472bd216-5d72-4e22-9524-e0401694dfa6"
      },
      "source": [
        "file_folder = ''  # the file folder or the dataset name\n",
        "\n",
        "GCS_DS_PATH = '/content/drive/MyDrive/ssid'\n",
        "\n",
        "if DEVICE == 'TPU':\n",
        "    if PLATFORM == 'COLAB':\n",
        "        files_train = np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))  # in this case it should be something like gs://\n",
        "else:\n",
        "    files_train = np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))\n",
        "    \n",
        "# Another way to do it if the files are already classify in folds is the following\n",
        "#for i in range(FOLDS):\n",
        "#    GCS_DS_PATH = KaggleDatasets().get_gcs_path('birdsongs-data-tf-external-fold'+str(i))\n",
        "#    files_train.append(np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))))    \n",
        "    \n",
        "    \n",
        "    \n",
        "train_df = pd.DataFrame({'path':files_train})\n",
        "train_df['fold'] = np.array([0,1,2,3,4])\n",
        "train_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/ssid/ssid_fold_0_000_25...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/ssid/ssid_fold_1_000_25...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/ssid/ssid_fold_2_000_25...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/ssid/ssid_fold_3_000_25...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/ssid/ssid_fold_4_000_25...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  fold\n",
              "0  /content/drive/MyDrive/ssid/ssid_fold_0_000_25...     0\n",
              "1  /content/drive/MyDrive/ssid/ssid_fold_1_000_25...     1\n",
              "2  /content/drive/MyDrive/ssid/ssid_fold_2_000_25...     2\n",
              "3  /content/drive/MyDrive/ssid/ssid_fold_3_000_25...     3\n",
              "4  /content/drive/MyDrive/ssid/ssid_fold_4_000_25...     4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNHsd8DV0fHu"
      },
      "source": [
        "### CV Strategy\n",
        "One of the most important things is to have a proper CV strategy, to make sure the CV result is reliable.  \n",
        "Usually, and when working with preprocessed TFRecords, the dataset is already split into folds.\n",
        "Usually the fold number can be found on the file name.  \n",
        "\n",
        "When experimenting, it is a good practice to have split the dataset beforehand, for reproducibility purposes.  \n",
        "\n",
        "If the dataset is not split yet, this is usually a good moment to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8gvo9ZU0fHw"
      },
      "source": [
        "### TFRecords Dataset Object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDiaK_Yjps0e",
        "papermill": {
          "duration": 0.013691,
          "end_time": "2020-09-11T15:27:13.409676",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.395985",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "TF Records is the fastest way to train using tensorflow. This avoids opening images or files individually, since many records can be added into the same file of 100-200 MB.   \n",
        "These are some basic functions and a schema to generate TFRecordDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:13.465927Z",
          "iopub.status.busy": "2020-09-11T15:27:13.445150Z",
          "iopub.status.idle": "2020-09-11T15:27:13.490886Z",
          "shell.execute_reply": "2020-09-11T15:27:13.490212Z"
        },
        "id": "g1rYIb9dpZKb",
        "papermill": {
          "duration": 0.067545,
          "end_time": "2020-09-11T15:27:13.490993",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.423448",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def read_labeled_tfrecord(ex):\n",
        "    \"\"\"\n",
        "    This is an example of decoding a tf record. You should know before hand the tf record format, and\n",
        "    define it in a dictionary.\n",
        "    \n",
        "    Inputs:\n",
        "        ex: is an tf example object, provided by the TFRecordDataset\n",
        "    Outputs:\n",
        "        data: the decoded data\n",
        "        label: the label of this example\n",
        "        \n",
        "    More parameters, inputs or outputs, can be added to this function.\n",
        "    \"\"\"\n",
        "    labeled_tfrec_format = {\n",
        "      'x': tf.io.FixedLenFeature([], tf.string), # image o data\n",
        "      'y': tf.io.FixedLenFeature([], tf.string), # label\n",
        "    }\n",
        "    example = tf.io.parse_single_example(ex, labeled_tfrec_format)\n",
        "    x = tf.io.decode_raw(example['x'], out_type=tf.float32)/255.\n",
        "    y = tf.io.decode_raw(example['y'], out_type=tf.float32)/255.\n",
        "\n",
        "    #y = tf.one_hot(y, n_classes, on_value=1.0, off_value=0.0, dtype=tf.float32) # labels in one hot format\n",
        "    return x, y # returns a decoded example \n",
        "\n",
        "def transforms(image, label, prob=0.5, dim=224):\n",
        "    # Data augmentation methods should come here\n",
        "    image = transform2d(image, dimension=dim, rotation=30.0, prob=prob)\n",
        "    image = tf.reshape(image, (dim,dim,3))\n",
        "    image = dropout(image, prob=prob, rank=2)\n",
        "    image = tf.reshape(image, (dim,dim,3))\n",
        "    return image, label\n",
        "\n",
        "def batch_transforms(batch, labels, batch_size, prob=0.5, dim=224):\n",
        "    # Data augmentation methods should come here\n",
        "    image2, label2 = cutmix(batch, labels, dimension=dim, prob=0.66, batch_size=batch_size, n_classes=5)\n",
        "    image3, label3 = mixup(batch, labels, dimension=dim, prob=0.66, batch_size=batch_size, n_classes=5)\n",
        "    imgs = []; labs = []\n",
        "    for j in range(batch_size):\n",
        "        P = tf.cast( tf.random.uniform([],0,1)<=0.5, tf.float32)\n",
        "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
        "        labs.append(P*label2[j,]+(1-P)*label3[j,])\n",
        "        \n",
        "    image4 = tf.reshape(tf.stack(imgs),(batch_size,dim,dim,3))\n",
        "    label4 = tf.reshape(tf.stack(labs),(batch_size,5))\n",
        "    return image4,label4\n",
        "\n",
        "\n",
        "def load_dataset(filenames, batch_size=32, labeled=True, shuffle=False, repeat=False, do_transforms=False, do_batch_transforms=False, drop_remainders=False):\n",
        "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
        "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
        "\n",
        "    # Create the dataset object from the filenames\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.cache()\n",
        "\n",
        "    # Repeats the dataset in a loop. Set true in training and false in validation\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "    \n",
        "    # Shuffle the dataset. True in training and false in validation\n",
        "    if shuffle: \n",
        "        dataset = dataset.shuffle(1024*REPLICAS)\n",
        "        opt = tf.data.Options()\n",
        "        opt.experimental_deterministic = False # disable order, increase speed\n",
        "        dataset = dataset.with_options(opt)\n",
        "\n",
        "    # At this point the dataset opens the files and reads TF Records\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO) # Decode TF Records\n",
        "\n",
        "    # At this point runs the transformations on the data, like data augmentation.\n",
        "    # transforms is a function which receives a sample and a label and returns a transformed sample and label\n",
        "    # this can be implemented in many ways\n",
        "    if do_transforms:\n",
        "        dataset = dataset.map(transforms, num_parallel_calls=AUTO)\n",
        "    \n",
        "    # For TPU the batches must have the same lenght, so it is mandatory to drop the remainders\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainders)\n",
        "    \n",
        "    # At this point runs tranforms that must be performed on batches, like mixup or cutmix\n",
        "    if do_batch_transforms:\n",
        "        dataset = dataset.map(lambda batch, label: augmentations_batch(batch, label, batch_size, prob, dim), num_parallel_calls=AUTO)\n",
        "    \n",
        "    # Whether to return the label or not\n",
        "    if labeled==False:\n",
        "        dataset = dataset.map(lambda image, label: image, num_parallel_calls=AUTO)\n",
        "    \n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    \"\"\"\n",
        "    The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    This is useful when calling the model.fit() method because it needs to know how many batches to run on the epoch\n",
        "    \"\"\" \n",
        "    n = [int(f[:f.rfind('.')].split('_')[-1]) for f in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "\n",
        "def get_fold(fold, train_df):\n",
        "    \"\"\"\n",
        "    This is an utility function to return the train and validation files to feed the dataset, given a fold number.\n",
        "    \n",
        "    Inputs:\n",
        "        fold: the fold number requested\n",
        "        train_df: a pandas DataFrame with a fold column and a path column\n",
        "        \n",
        "    Outputs:\n",
        "        train_files: an array with the training files of this fold\n",
        "        val_files: an array with the validation files of this fold\n",
        "        \"\"\"\n",
        "    train_files = train_df[train_df['fold']!=str(fold)]['path'].values\n",
        "    val_files = train_df[train_df['fold']==str(fold)]['path'].values\n",
        "    return train_files, val_files"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYwW_MIXwuW5",
        "outputId": "84dbdb20-af0e-45d5-923e-c7d0b8ee5afa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "files_train, files_test = get_fold(0, train_df)\n",
        "files_train, files_test"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['/content/drive/MyDrive/ssid/ssid_fold_0_000_256.tfrec',\n",
              "        '/content/drive/MyDrive/ssid/ssid_fold_1_000_256.tfrec',\n",
              "        '/content/drive/MyDrive/ssid/ssid_fold_2_000_256.tfrec',\n",
              "        '/content/drive/MyDrive/ssid/ssid_fold_3_000_256.tfrec',\n",
              "        '/content/drive/MyDrive/ssid/ssid_fold_4_000_256.tfrec'],\n",
              "       dtype=object), array([], dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Y3OXOPwkue",
        "outputId": "9fe4b643-e2f4-4cf5-8b27-e1b51d5bb25f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "source": [
        "ds = load_dataset(files_train, batch_size=BS_TRAIN[i]*REPLICAS, labeled=True, shuffle=True, repeat=True, drop_remainders=True)\n",
        "for b in ds.take(1):\n",
        "    print(b)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnimplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2263\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    243\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m       element_spec, tensor_list)\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    242\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 243\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m       element_spec, tensor_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m   1236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: '/content/drive/MyDrive/ssid/ssid_fold_0_000_256.tfrec')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-240a003973cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS_TRAIN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mREPLICAS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2267\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnimplementedError\u001b[0m: File system scheme '[local]' not implemented (file: '/content/drive/MyDrive/ssid/ssid_fold_0_000_256.tfrec')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVhw7qHPskjC",
        "papermill": {
          "duration": 0.013884,
          "end_time": "2020-09-11T15:27:13.519691",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.505807",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Model\n",
        "When experimenting, many different models or variations can be tried.  \n",
        "It is useful to have a common function to route the model creations further in the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:13.561378Z",
          "iopub.status.busy": "2020-09-11T15:27:13.556079Z",
          "iopub.status.idle": "2020-09-11T15:27:13.600008Z",
          "shell.execute_reply": "2020-09-11T15:27:13.599467Z"
        },
        "id": "z76lprEbsnwO",
        "papermill": {
          "duration": 0.066368,
          "end_time": "2020-09-11T15:27:13.600116",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.533748",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
        "\n",
        "# Example of model based on efficient net with categorical crossentropy\n",
        "def get_model(shape=(224,224,3)):\n",
        "\n",
        "    # inputs\n",
        "    inp = tf.keras.layers.Input(shape=shape)\n",
        "    base = models.ViT_UNet(depth = 2,\n",
        "                    depth_te = 1,\n",
        "                    size_bottleneck = 2,\n",
        "                    preprocessing = 'conv',\n",
        "                    num_patches=196,\n",
        "                    patch_size = 16,\n",
        "                    num_channels = 3,\n",
        "                    hidden_dim = 64,\n",
        "                    num_heads = 4,\n",
        "                    attn_drop = 0.2,\n",
        "                    proj_drop = 0.2,\n",
        "                    linear_drop = 0,\n",
        "                    )\n",
        "    x = base(inp)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001*REPLICAS)\n",
        "    loss = tf.keras.losses.MeanSquaredError() \n",
        "\n",
        "    model.compile(optimizer=opt, \n",
        "                  loss=loss,\n",
        "                  metrics=[psnr])  # some extra custom metrics\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfOAufbov_CJ",
        "papermill": {
          "duration": 0.013626,
          "end_time": "2020-09-11T15:27:13.627569",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.613943",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Experiments Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:13.664986Z",
          "iopub.status.busy": "2020-09-11T15:27:13.664308Z",
          "iopub.status.idle": "2020-09-11T15:27:13.667187Z",
          "shell.execute_reply": "2020-09-11T15:27:13.666681Z"
        },
        "id": "47XYJuRhwBJ4",
        "papermill": {
          "duration": 0.026181,
          "end_time": "2020-09-11T15:27:13.667279",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.641098",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "N_EXPERIMENTS = 1  # Normally not more than one run per commit\n",
        "FOLDS = [0,1,2,3,4]  # Each run should cover a single fold\n",
        "\n",
        "# DATASET PARAMS\n",
        "#IMG_SIZE = [128] * N_EXPERIMENTS\n",
        "\n",
        "# DATALOADER PARAMS\n",
        "BS_TRAIN = [8] * N_EXPERIMENTS\n",
        "BS_VAL = [8] * N_EXPERIMENTS\n",
        "\n",
        "# TRANSFORMS\n",
        "# Params for the transforms functions\n",
        "\n",
        "# GLOBAL PARAMETERS\n",
        "EPOCHS=20\n",
        "DISPLAY_PLOT=True\n",
        "VERBOSE = 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhKaxmyev1kL",
        "papermill": {
          "duration": 0.013696,
          "end_time": "2020-09-11T15:27:13.695337",
          "exception": false,
          "start_time": "2020-09-11T15:27:13.681641",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-11T15:27:23.982048Z",
          "iopub.status.busy": "2020-09-11T15:27:23.972100Z",
          "iopub.status.idle": "2020-09-11T16:02:52.132057Z",
          "shell.execute_reply": "2020-09-11T16:02:52.127652Z"
        },
        "id": "T5e8mXKQxYuq",
        "papermill": {
          "duration": 2128.196861,
          "end_time": "2020-09-11T16:02:52.132185",
          "exception": false,
          "start_time": "2020-09-11T15:27:23.935324",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a04c5414-f72f-417b-f0af-6a0acb5d39c2"
      },
      "source": [
        "seed_everything(42)\n",
        "\n",
        "for i in range(0,N_EXPERIMENTS):\n",
        "    print(f'********** EXPERIMENT {i} **********')\n",
        "    print(f'***** bs train {BS_TRAIN[i]*REPLICAS} *****')\n",
        "    print(f'***** bs val {BS_VAL[i]*REPLICAS} *****')\n",
        "    print(f'**********************************\\n')\n",
        "\n",
        "    # INIT TPU\n",
        "    if DEVICE=='TPU':\n",
        "        init_tpu(tpu)\n",
        "    \n",
        "    # CREATE TRAIN AND VALIDATION DATASETS\n",
        "    files_train, files_val = get_fold(FOLDS[i], train_df)\n",
        "    print(f\"{files_train} files in train and {files_val} files in validation\")\n",
        "\n",
        "    # DATASETS\n",
        "    val_dataset = load_dataset(files_val, batch_size=BS_VAL[i]*REPLICAS, labeled=True, shuffle=False, repeat=False, drop_remainders=True ),\n",
        "    \n",
        "    # BUILD MODEL\n",
        "    print('Building model...')\n",
        "    K.clear_session()\n",
        "    with strategy.scope():\n",
        "        model = get_model()\n",
        "\n",
        "    # SAVE BEST MODEL EACH FOLD\n",
        "    model_path = \"fold\"+str(i)\n",
        "    \n",
        "    # CALLBACKS\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(model_path+'.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=0, mode='min', min_delta=0.0001, cooldown=0, min_lr=1e-8)\n",
        "\n",
        "    # TRAIN\n",
        "    print('Training...')\n",
        "    history = model.fit(\n",
        "        load_dataset(files_train, batch_size=BS_TRAIN[i]*REPLICAS, labeled=True, shuffle=True, repeat=True, drop_remainders=True),\n",
        "        epochs = EPOCHS, \n",
        "        callbacks = [es,sv,lr],\n",
        "        steps_per_epoch = count_data_items(files_train)/BS_TRAIN[i]//REPLICAS,\n",
        "        validation_data = val_dataset,\n",
        "        verbose = VERBOSE\n",
        "    )\n",
        "    \n",
        "    # PLOT TRAINING\n",
        "    if DISPLAY_PLOT:\n",
        "        history = pd.DataFrame(history.history)\n",
        "        plt.figure(figsize=(15,5))\n",
        "        plt.plot(np.arange(len(history)), history['loss'],'-o',label='Train Loss',color='#ff7f0e')\n",
        "        plt.plot(np.arange(len(history)), history['val_loss'],'-o',label='Val Loss',color='#1f77b4')\n",
        "        x = np.argmin( history['val_loss'] ); y = np.min( history['val_loss'] )\n",
        "        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "        plt.text(x-0.03*xdist,y-0.13*ydist,'min loss\\n%.2f'%y,size=14)\n",
        "        plt.ylabel('Loss',size=14); plt.xlabel('Epoch',size=14)\n",
        "        plt.legend(loc=2)\n",
        "        \n",
        "        plt.title('Experiment %i'%i,size=18)\n",
        "        plt.legend(loc=3)\n",
        "        plt.show()\n",
        "    \n",
        "    print('\\n')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********** EXPERIMENT 0 **********\n",
            "***** bs train 64 *****\n",
            "***** bs val 64 *****\n",
            "**********************************\n",
            "\n",
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.71.20.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.71.20.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.20.250:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.71.20.250:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/ssid/ssid_fold_0_000_256.tfrec'\n",
            " '/content/drive/MyDrive/ssid/ssid_fold_1_000_256.tfrec'\n",
            " '/content/drive/MyDrive/ssid/ssid_fold_2_000_256.tfrec'\n",
            " '/content/drive/MyDrive/ssid/ssid_fold_3_000_256.tfrec'\n",
            " '/content/drive/MyDrive/ssid/ssid_fold_4_000_256.tfrec'] files in train and [] files in validation\n",
            "Building model...\n",
            "Architecture information:\n",
            "Level 0:\n",
            "\tPatch size: 16\n",
            "\tNum. patches: 196\n",
            "\tProjection size: 768\n",
            "\tHidden dim. size: 64\n",
            "Level 1:\n",
            "\tPatch size: 8\n",
            "\tNum. patches: 784\n",
            "\tProjection size: 192\n",
            "\tHidden dim. size: 32\n",
            "Level 2:\n",
            "\tPatch size: 4\n",
            "\tNum. patches: 3136\n",
            "\tProjection size: 48\n",
            "\tHidden dim. size: 16\n",
            "Training...\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (8, None).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (8, None).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0990b88e4114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_data_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBS_TRAIN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mREPLICAS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVERBOSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-34-cd2d9fb6d581>:4 psnr  *\n        return tf.image.psnr(y_true, y_pred, max_val=1.0)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/image_ops_impl.py:4064 psnr\n        mse = math_ops.reduce_mean(math_ops.squared_difference(a, b), [-3, -2, -1])\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:2583 reduce_mean\n        name=name))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py:5955 mean\n        name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:601 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3569 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2042 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Invalid reduction dimension -3 for input with 2 dimensions. for '{{node PSNR/Mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](PSNR/SquaredDifference, PSNR/Mean/reduction_indices)' with input shapes: [8,?], [3] and with computed input tensors: input[1] = <-3 -2 -1>.\n"
          ]
        }
      ]
    }
  ]
}