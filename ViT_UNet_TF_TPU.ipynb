{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow Experiments Template"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.01304,
     "end_time": "2020-09-11T15:26:41.959908",
     "exception": false,
     "start_time": "2020-09-11T15:26:41.946868",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# install benatools library\n",
    "!pip install benatools >> /dev/null\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import os\n",
    "import time as time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from benatools.tf.metrics import (true_positives, possible_positives, predicted_positives, F1, recall, precision)\n",
    "from benatools.tf.tpu import (get_device_strategy, init_tpu)\n",
    "from benatools.utils.tools import MultiStratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# CONSTANTS\n",
    "PLATFORM = 'KAGGLE'  # this could be 'COLAB' or 'LOCAL'\n",
    "DEVICE = 'TPU'   # This could be 'GPU' or 'CPU'"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:26:41.995909Z",
     "iopub.status.busy": "2020-09-11T15:26:41.995127Z",
     "iopub.status.idle": "2020-09-11T15:27:04.983364Z",
     "shell.execute_reply": "2020-09-11T15:27:04.982583Z"
    },
    "id": "tI1dM5_TnFCG",
    "outputId": "4ba829dc-c73e-4aa0-aff4-71b566eed92f",
    "papermill": {
     "duration": 23.012195,
     "end_time": "2020-09-11T15:27:04.983488",
     "exception": false,
     "start_time": "2020-09-11T15:26:41.971293",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Seeding everything for experiment replicability"
   ],
   "metadata": {
    "id": "IhfkH4zcYy-J",
    "papermill": {
     "duration": 0.01142,
     "end_time": "2020-09-11T15:27:05.007150",
     "exception": false,
     "start_time": "2020-09-11T15:27:04.995730",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:05.036809Z",
     "iopub.status.busy": "2020-09-11T15:27:05.035935Z",
     "iopub.status.idle": "2020-09-11T15:27:05.038842Z",
     "shell.execute_reply": "2020-09-11T15:27:05.038202Z"
    },
    "id": "Uz6eDyk4nQ0z",
    "papermill": {
     "duration": 0.019834,
     "end_time": "2020-09-11T15:27:05.038937",
     "exception": false,
     "start_time": "2020-09-11T15:27:05.019103",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up distributed strategy. In case of training with TPU's or multiple GPU's, a distributed strategy must be created. "
   ],
   "metadata": {
    "id": "4Bimv-klYwEh",
    "papermill": {
     "duration": 0.011595,
     "end_time": "2020-09-11T15:27:05.062538",
     "exception": false,
     "start_time": "2020-09-11T15:27:05.050943",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strategy, AUTO, REPLICAS, tpu = get_device_strategy(DEVICE, verbose=True)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:05.182608Z",
     "iopub.status.busy": "2020-09-11T15:27:05.101411Z",
     "iopub.status.idle": "2020-09-11T15:27:10.039000Z",
     "shell.execute_reply": "2020-09-11T15:27:10.038440Z"
    },
    "id": "8H0wCAbEo1UZ",
    "outputId": "75ec3ddd-894e-4ffb-a8d4-658a644ae167",
    "papermill": {
     "duration": 4.96508,
     "end_time": "2020-09-11T15:27:10.039112",
     "exception": false,
     "start_time": "2020-09-11T15:27:05.074032",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are normally some files linked to the dataset with metadata, contextual information, calendars, etc."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Read files\n",
    "# training_examples = pd.read_csv('training_examples.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "id": "mOd-aL1EmNcd",
    "papermill": {
     "duration": 0.012503,
     "end_time": "2020-09-11T15:27:10.131456",
     "exception": false,
     "start_time": "2020-09-11T15:27:10.118953",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Folder\n",
    "If training on TPU, the data must be stored into a GS bucket.  \n",
    "When training on Kaggle platform, calling \n",
    "```python\n",
    "KaggleDatasets().get_gcs_path() \n",
    "```\n",
    "automatically copies the dataset into a GS bucket.  \n",
    "If training on Google Colab, be aware that you might incurr in egress charges.  \n",
    "If training on GPU, there is no such problem."
   ],
   "metadata": {
    "id": "oxXWUyVSnyN5",
    "papermill": {
     "duration": 0.012948,
     "end_time": "2020-09-11T15:27:10.158651",
     "exception": false,
     "start_time": "2020-09-11T15:27:10.145703",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "file_folder = ''  # the file folder or the dataset name\n",
    "\n",
    "if DEVICE == 'TPU':\n",
    "    if PLATFORM == 'KAGGLE':\n",
    "        from kaggle_datasets import KaggleDatasets\n",
    "        GCS_DS_PATH = KaggleDatasets().get_gcs_path('birdsongs-data-tf-external-fold'+str(i))\n",
    "        files_train = np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))\n",
    "    if PLATFORM == 'COLAB':\n",
    "        files_train = np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))  # in this case it should be something like gs://\n",
    "else:\n",
    "    files_train = np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))\n",
    "    \n",
    "# Another way to do it if the files are already classify in folds is the following\n",
    "#for i in range(FOLDS):\n",
    "#    GCS_DS_PATH = KaggleDatasets().get_gcs_path('birdsongs-data-tf-external-fold'+str(i))\n",
    "#    files_train.append(np.sort(np.array(tf.io.gfile.glob(GCS_DS_PATH + '/*.tfrec'))))    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_df = pd.DataFrame({'path':files_train})"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:10.204122Z",
     "iopub.status.busy": "2020-09-11T15:27:10.191955Z",
     "iopub.status.idle": "2020-09-11T15:27:13.381441Z",
     "shell.execute_reply": "2020-09-11T15:27:13.381916Z"
    },
    "id": "Q98vpflUmQM8",
    "outputId": "dc4e1846-4593-4f40-e58c-67e05e9a9ff8",
    "papermill": {
     "duration": 3.210706,
     "end_time": "2020-09-11T15:27:13.382046",
     "exception": false,
     "start_time": "2020-09-11T15:27:10.171340",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CV Strategy\n",
    "One of the most important things is to have a proper CV strategy, to make sure the CV result is reliable.  \n",
    "Usually, and when working with preprocessed TFRecords, the dataset is already split into folds.\n",
    "Usually the fold number can be found on the file name.  \n",
    "\n",
    "When experimenting, it is a good practice to have split the dataset beforehand, for reproducibility purposes.  \n",
    "\n",
    "If the dataset is not split yet, this is usually a good moment to do it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# If the dataset is already split into folds, these lines could help\n",
    "#train_df['fold'] = train_df['path'].str.split('/')\n",
    "#train_df['fold'] = train_df['fold'].apply(lambda x:x[-1].split('_')[1])\n",
    "#train_df.groupby('fold').count()\n",
    "\n",
    "# This is a good point to merge the files with the metadata dataset, if any\n",
    "train_df = train_df.merge(training_examples, on='id')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "cv = MultiStratifiedKFold(n_splits=5)\n",
    "train_df['fold'] = -1\n",
    "for fold, (train_idx, val_idx) in cv.split():\n",
    "    train_df.iloc[val_idx]['fold'] = fold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TFRecords Dataset Object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF Records is the fastest way to train using tensorflow. This avoids opening images or files individually, since many records can be added into the same file of 100-200 MB.   \n",
    "These are some basic functions and a schema to generate TFRecordDataset"
   ],
   "metadata": {
    "id": "BDiaK_Yjps0e",
    "papermill": {
     "duration": 0.013691,
     "end_time": "2020-09-11T15:27:13.409676",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.395985",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "def read_labeled_tfrecord(ex):\n",
    "    \"\"\"\n",
    "    This is an example of decoding a tf record. You should know before hand the tf record format, and\n",
    "    define it in a dictionary.\n",
    "    \n",
    "    Inputs:\n",
    "        ex: is an tf example object, provided by the TFRecordDataset\n",
    "    Outputs:\n",
    "        data: the decoded data\n",
    "        label: the label of this example\n",
    "        \n",
    "    More parameters, inputs or outputs, can be added to this function.\n",
    "    \"\"\"\n",
    "    labeled_tfrec_format = {\n",
    "      'image': tf.io.FixedLenFeature([], tf.string), # image o data\n",
    "      'y':  tf.io.FixedLenFeature([], tf.int64), # label\n",
    "    }\n",
    "    example = tf.io.parse_single_example(ex, labeled_tfrec_format)\n",
    "    image = tf.io.decode_raw(example['image'], out_type=tf.float32)\n",
    "    y = tf.cast(example['y'], tf.int32)\n",
    "\n",
    "    #y = tf.one_hot(y, n_classes, on_value=1.0, off_value=0.0, dtype=tf.float32) # labels in one hot format\n",
    "    return image, y # returns a decoded example \n",
    "\n",
    "def transforms(image, label, prob=0.5, dim=224):\n",
    "    # Data augmentation methods should come here\n",
    "    image = transform2d(image, dimension=dim, rotation=30.0, prob=prob)\n",
    "    image = tf.reshape(image, (dim,dim,3))\n",
    "    image = dropout(image, prob=prob, rank=2)\n",
    "    image = tf.reshape(image, (dim,dim,3))\n",
    "    return image, label\n",
    "\n",
    "def batch_transforms(batch, labels, batch_size, prob=0.5, dim=224):\n",
    "    # Data augmentation methods should come here\n",
    "    image2, label2 = cutmix(batch, labels, dimension=dim, prob=0.66, batch_size=batch_size, n_classes=5)\n",
    "    image3, label3 = mixup(batch, labels, dimension=dim, prob=0.66, batch_size=batch_size, n_classes=5)\n",
    "    imgs = []; labs = []\n",
    "    for j in range(batch_size):\n",
    "        P = tf.cast( tf.random.uniform([],0,1)<=0.5, tf.float32)\n",
    "        imgs.append(P*image2[j,]+(1-P)*image3[j,])\n",
    "        labs.append(P*label2[j,]+(1-P)*label3[j,])\n",
    "        \n",
    "    image4 = tf.reshape(tf.stack(imgs),(batch_size,dim,dim,3))\n",
    "    label4 = tf.reshape(tf.stack(labs),(batch_size,5))\n",
    "    return image4,label4\n",
    "\n",
    "\n",
    "def load_dataset(filenames, batch_size=32, labeled=True, shuffle=False, repeat=False, do_transforms=False, do_batch_transforms=False, drop_remainders=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    # Create the dataset object from the filenames\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # Repeats the dataset in a loop. Set true in training and false in validation\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    \n",
    "    # Shuffle the dataset. True in training and false in validation\n",
    "    if shuffle: \n",
    "        dataset = dataset.shuffle(1024*REPLICAS)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False # disable order, increase speed\n",
    "        dataset = dataset.with_options(opt)\n",
    "\n",
    "    # At this point the dataset opens the files and reads TF Records\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO) # Decode TF Records\n",
    "\n",
    "    # At this point runs the transformations on the data, like data augmentation.\n",
    "    # transforms is a function which receives a sample and a label and returns a transformed sample and label\n",
    "    # this can be implemented in many ways\n",
    "    if do_transforms:\n",
    "        dataset = dataset.map(transforms, num_parallel_calls=AUTO)\n",
    "    \n",
    "    # For TPU the batches must have the same lenght, so it is mandatory to drop the remainders\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=drop_remainders)\n",
    "    \n",
    "    # At this point runs tranforms that must be performed on batches, like mixup or cutmix\n",
    "    if do_batch_transforms:\n",
    "        dataset = dataset.map(lambda batch, label: augmentations_batch(batch, label, batch_size, prob, dim), num_parallel_calls=AUTO)\n",
    "    \n",
    "    # Whether to return the label or not\n",
    "    if labeled==False:\n",
    "        dataset = dataset.map(lambda image, label: image, num_parallel_calls=AUTO)\n",
    "    \n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    \"\"\"\n",
    "    The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    This is useful when calling the model.fit() method because it needs to know how many batches to run on the epoch\n",
    "    \"\"\" \n",
    "    n = [int(f[:f.rfind('.')].split('_')[-1]) for f in filenames]\n",
    "    return np.sum(n)\n",
    "\n",
    "\n",
    "def get_fold(fold, train_df):\n",
    "    \"\"\"\n",
    "    This is an utility function to return the train and validation files to feed the dataset, given a fold number.\n",
    "    \n",
    "    Inputs:\n",
    "        fold: the fold number requested\n",
    "        train_df: a pandas DataFrame with a fold column and a path column\n",
    "        \n",
    "    Outputs:\n",
    "        train_files: an array with the training files of this fold\n",
    "        val_files: an array with the validation files of this fold\n",
    "        \"\"\"\n",
    "    train_files = train_df[train_df['fold']!=str(fold)]['path'].values\n",
    "    val_files = train_df[train_df['fold']==str(fold)]['path'].values\n",
    "    return train_files, val_files"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:13.465927Z",
     "iopub.status.busy": "2020-09-11T15:27:13.445150Z",
     "iopub.status.idle": "2020-09-11T15:27:13.490886Z",
     "shell.execute_reply": "2020-09-11T15:27:13.490212Z"
    },
    "id": "g1rYIb9dpZKb",
    "papermill": {
     "duration": 0.067545,
     "end_time": "2020-09-11T15:27:13.490993",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.423448",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "When experimenting, many different models or variations can be tried.  \n",
    "It is useful to have a common function to route the model creations further in the training loop"
   ],
   "metadata": {
    "id": "XVhw7qHPskjC",
    "papermill": {
     "duration": 0.013884,
     "end_time": "2020-09-11T15:27:13.519691",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.505807",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to be called from the training loop\n",
    "# This function calls the functions that actually return a compiled model\n",
    "def get_model(model_id, b, n_classes, shape):\n",
    "    if model_id == 0:\n",
    "        return get_model0(b, n_classes, shape)\n",
    "    # mode models to add here\n",
    "\n",
    "\n",
    "# Example of model based on efficient net with categorical crossentropy\n",
    "def get_model0(b, n_classes, shape=(128,128,3)):\n",
    "\n",
    "    # inputs\n",
    "    inp = tf.keras.layers.Input(shape=shape)\n",
    "    \n",
    "    base = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet', input_shape=shape)\n",
    "\n",
    "    x = base(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # # use the same head as the baseline notebook.\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "    x = tf.keras.layers.Dense(n_classes, activation='softmax')(x) # softmax converts logits (raw predictions [-inf,+inf]) to probabilities [0,1]\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001*REPLICAS)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy( label_smoothing=0.05) \n",
    "    #loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.05) \n",
    "\n",
    "    model.compile(optimizer=opt, \n",
    "                  #experimental_steps_per_execution=5,  #experimental\n",
    "                  loss=loss,\n",
    "                  metrics=[F1,true_positives,possible_positives,predicted_positives, recall, precission])  # some extra custom metrics\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:13.561378Z",
     "iopub.status.busy": "2020-09-11T15:27:13.556079Z",
     "iopub.status.idle": "2020-09-11T15:27:13.600008Z",
     "shell.execute_reply": "2020-09-11T15:27:13.599467Z"
    },
    "id": "z76lprEbsnwO",
    "papermill": {
     "duration": 0.066368,
     "end_time": "2020-09-11T15:27:13.600116",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.533748",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments Configuration"
   ],
   "metadata": {
    "id": "QfOAufbov_CJ",
    "papermill": {
     "duration": 0.013626,
     "end_time": "2020-09-11T15:27:13.627569",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.613943",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "N_EXPERIMENTS = 5  # Normally not more than one run per commit\n",
    "FOLDS = [0,1,2,3,4]  # Each run should cover a single fold\n",
    "\n",
    "# DATASET PARAMS\n",
    "#IMG_SIZE = [128] * N_EXPERIMENTS\n",
    "\n",
    "# DATALOADER PARAMS\n",
    "BS_TRAIN = [128] * N_EXPERIMENTS\n",
    "BS_VAL = [128] * N_EXPERIMENTS\n",
    "\n",
    "# MODEL PARAMS\n",
    "MODEL = [2] * N_EXPERIMENTS\n",
    "B = [1] * N_EXPERIMENTS\n",
    "\n",
    "# TRANSFORMS\n",
    "# Params for the transforms functions\n",
    "\n",
    "# GLOBAL PARAMETERS\n",
    "EPOCHS=20\n",
    "DISPLAY_PLOT=True\n",
    "VERBOSE = 1"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:13.664986Z",
     "iopub.status.busy": "2020-09-11T15:27:13.664308Z",
     "iopub.status.idle": "2020-09-11T15:27:13.667187Z",
     "shell.execute_reply": "2020-09-11T15:27:13.666681Z"
    },
    "id": "47XYJuRhwBJ4",
    "papermill": {
     "duration": 0.026181,
     "end_time": "2020-09-11T15:27:13.667279",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.641098",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Loop"
   ],
   "metadata": {
    "id": "KhKaxmyev1kL",
    "papermill": {
     "duration": 0.013696,
     "end_time": "2020-09-11T15:27:13.695337",
     "exception": false,
     "start_time": "2020-09-11T15:27:13.681641",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "seed_everything(42)\n",
    "\n",
    "for i in range(0,N_EXPERIMENTS-4):\n",
    "    print(f'********** EXPERIMENT {i} **********')\n",
    "    print(f'***** bs train {BS_TRAIN[i]*REPLICAS} *****')\n",
    "    print(f'***** bs val {BS_VAL[i]*REPLICAS} *****')\n",
    "    print(f'***** model {MODEL[i]} *****')\n",
    "    print(f'***** efficientnet B{B[i]} *****')\n",
    "    print(f'***** dropout rate {P_DROPOUT[i]} *****')\n",
    "    print(f'***** noise rate {P_NOISE[i]} *****')\n",
    "    print(f'**********************************\\n')\n",
    "\n",
    "    # INIT TPU\n",
    "    if DEVICE=='TPU':\n",
    "        init_tpu(tpu)\n",
    "    \n",
    "    # CREATE TRAIN AND VALIDATION DATASETS\n",
    "    files_train, files_val = get_fold(FOLDS[i], train_all)\n",
    "\n",
    "    # DATASETS\n",
    "    val_dataset = load_dataset(files_val, device=device, batch_size=BS_VAL[i]*REPLICAS, labeled=True, shuffle=False, repeat=False),\n",
    "    \n",
    "    # BUILD MODEL\n",
    "    print('Building model...')\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = get_model(MODEL[i], B[i], N_CLASSES, shape=shape)\n",
    "\n",
    "    # SAVE BEST MODEL EACH FOLD\n",
    "    model_path = \"fold\"+str(i)\n",
    "    \n",
    "    # CALLBACKS\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(model_path+'.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, verbose=0, mode='min', min_delta=0.0001, cooldown=0, min_lr=1e-8)\n",
    "\n",
    "    # TRAIN\n",
    "    print('Training...')\n",
    "    history = model.fit(\n",
    "        load_dataset(files_train, device=device, batch_size=BS_TRAIN[i]*REPLICAS, labeled=True, shuffle=True, repeat=True, transforms=transforms),\n",
    "        epochs = EPOCHS, \n",
    "        callbacks = [es,sv,lr],\n",
    "        steps_per_epoch = count_data_items(files_train)/BS_TRAIN[i]//REPLICAS,\n",
    "        validation_data = val_dataset,\n",
    "        verbose = VERBOSE\n",
    "    )\n",
    "    \n",
    "    # PLOT TRAINING\n",
    "    if DISPLAY_PLOT:\n",
    "        history = pd.DataFrame(history.history)\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(np.arange(len(history)), history['loss'],'-o',label='Train Loss',color='#ff7f0e')\n",
    "        plt.plot(np.arange(len(history)), history['val_loss'],'-o',label='Val Loss',color='#1f77b4')\n",
    "        x = np.argmin( history['val_loss'] ); y = np.min( history['val_loss'] )\n",
    "        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
    "        plt.text(x-0.03*xdist,y-0.13*ydist,'min loss\\n%.2f'%y,size=14)\n",
    "        plt.ylabel('Loss',size=14); plt.xlabel('Epoch',size=14)\n",
    "        plt.legend(loc=2)\n",
    "        \n",
    "        plt2 = plt.gca().twinx()\n",
    "        plt2.plot(np.arange(len(history)),history['F1'],'-o',label='Train F1',color='#36de47')\n",
    "        plt2.plot(np.arange(len(history)),history['val_F1'],'-o',label='Val F1',color='#330066')\n",
    "        #x = np.argmax( history['val_F1'] ); y = np.max( history['val_F1'] )\n",
    "        #xdist = plt2.xlim()[1] - plt2.xlim()[0]; ydist = plt2.ylim()[1] - plt2.ylim()[0]\n",
    "        #plt2.text(x-0.03*xdist,y-0.13*ydist,'max F1\\n%.2f'%y,size=14)\n",
    "        #plt2.ylabel('F1',size=14); plt2.xlabel('Epoch',size=14)\n",
    "        plt2.legend()\n",
    "        \n",
    "        #plt2 = plt.gca().twinx()\n",
    "        #plt2.plot(np.arange(len(history)),history['lr'],'-o',label='LR',color='#2ca02c')\n",
    "        #plt.ylabel('LR',size=14)\n",
    "        \n",
    "        plt.title('Experiment %i'%i,size=18)\n",
    "        plt.legend(loc=3)\n",
    "        plt.show()\n",
    "    \n",
    "    print('\\n')"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-11T15:27:23.982048Z",
     "iopub.status.busy": "2020-09-11T15:27:23.972100Z",
     "iopub.status.idle": "2020-09-11T16:02:52.132057Z",
     "shell.execute_reply": "2020-09-11T16:02:52.127652Z"
    },
    "id": "T5e8mXKQxYuq",
    "outputId": "67b004e3-dcba-4be3-a8a1-104e7db614fd",
    "papermill": {
     "duration": 2128.196861,
     "end_time": "2020-09-11T16:02:52.132185",
     "exception": false,
     "start_time": "2020-09-11T15:27:23.935324",
     "status": "completed"
    },
    "tags": []
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 2177.718488,
   "end_time": "2020-09-11T16:02:55.111435",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-11T15:26:37.392947",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}