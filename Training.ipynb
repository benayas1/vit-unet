{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/benayas1/vit-unet/blob/main/Training.ipynb",
      "authorship_tag": "ABX9TyOdJ/VayDsX4AtoNRWyqTA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benayas1/vit-unet/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6EO9Llrk7Pr",
        "outputId": "6189ea27-105d-49fb-aed7-d0719e4b6ae4"
      },
      "source": [
        "%%script bash\n",
        "\n",
        "git clone https://benayas1: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@github.com/benayas1/vit-unet.git >> /dev/null\n",
        "(cd /content/vit-unet/ && python setup.py bdist_wheel && pip install dist/vit_unet-0.0.1-py3-none-any.whl) >> /dev/null\n",
        "\n",
        "pip install fire >> /dev/null\n",
        "pip install wandb >> /dev/null\n",
        "pip install benatools >> /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'vit-unet'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRp-Xu1coUXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23ff1bb-d17f-4970-88d2-77bb14e9c47f"
      },
      "source": [
        "import fire\n",
        "import vit_unet.torch.model as models\n",
        "import vit_unet.torch.functions as fn\n",
        "import vit_unet.torch.dataset as dataset\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import KFold\n",
        "import albumentations\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import wandb\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "import matplotlib.pyplot as plt\n",
        "from benatools.torch.fitter import ImageFitter\n",
        "print(torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9.0+cu102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-8OIOcSKmfu"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "C7Zwak8mscOg",
        "outputId": "5feb8942-fecb-4b89-a24f-b5ed539b7af6"
      },
      "source": [
        "input_folder='/content/drive/MyDrive/ssid'\n",
        "n_epochs=1\n",
        "folds=5\n",
        "model_string='lite'\n",
        "lr=0.0001\n",
        "batch_size=4\n",
        "im_size=224\n",
        "\n",
        "torch.random.manual_seed(42)\n",
        "\n",
        "WB_ENTITY = 'UAL'\n",
        "wandb.login(key='ab1f4c380e0a008223b6434a42907bacfd7b4e26') # WANDB KEY\n",
        "with wandb.init(project='ViT_UNet', entity=WB_ENTITY) as run:\n",
        "\n",
        "    wandb.config.update({'n_epochs':n_epochs,\n",
        "                         'fold':folds,\n",
        "                         'model':model_string,\n",
        "                         'lr':lr\n",
        "                         })\n",
        "    \n",
        "    # prepare Data\n",
        "    clean = np.array([path.split('/')[-1][:-4] for path in sorted(glob(os.path.join(input_folder,'clean','*')))])\n",
        "    noisy = np.array([path.split('/')[-1][:-4] for path in sorted(glob(os.path.join(input_folder,'noisy','*')))])\n",
        "        \n",
        "    clean = np.array([path for path in clean if path in noisy])\n",
        "\n",
        "    assert len(clean)==len(noisy), f\"Clean length {len(clean)} is not equal to Noisy length {len(noisy)}\"\n",
        "\n",
        "    cv = KFold(folds, shuffle=False, random_state=42)\n",
        "    results = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(cv.split(noisy)):\n",
        "        train = noisy[train_idx]\n",
        "        test = noisy[test_idx]\n",
        "\n",
        "        print(f'FOLD {fold}: Training on {len(train)} samples and testing on {len(test)} samples')\n",
        "        # Create dataset and dataloader\n",
        "        train_transform = albumentations.Compose([\n",
        "            albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, border_mode=cv2.BORDER_CONSTANT, p=1.0),\n",
        "            #albumentations.Normalize(mean=(0.456), std=(0.224), max_pixel_value=255.0, p=1.0)\n",
        "        ])\n",
        "\n",
        "        val_transform = albumentations.Compose([\n",
        "            albumentations.Normalize(mean=(0.456), std=(0.224), max_pixel_value=255.0, p=1.0)\n",
        "        ])\n",
        "        train_dataloader = torch.utils.data.DataLoader(dataset.DenoisingDataset(train, \n",
        "                                                                                clean_folder=os.path.join(input_folder,'clean'), \n",
        "                                                                                noisy_folder=os.path.join(input_folder,'noisy'), \n",
        "                                                                                augments=train_transform,\n",
        "                                                                                im_size=im_size),\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        shuffle=True,\n",
        "                                                        num_workers=2)\n",
        "        val_dataloader = torch.utils.data.DataLoader(dataset.DenoisingDataset(test, \n",
        "                                                                               clean_folder=os.path.join(input_folder,'clean'), \n",
        "                                                                               noisy_folder=os.path.join(input_folder,'noisy'), \n",
        "                                                                               #augments=val_transform,\n",
        "                                                                               im_size=im_size),\n",
        "                                                      batch_size=batch_size,\n",
        "                                                      shuffle=False,\n",
        "                                                      num_workers=2)\n",
        "\n",
        "        # Create model\n",
        "        #model = models.get_vit_unet(model_string, verbose=False)\n",
        "        model = models.ViT_UNet(depth = 2,\n",
        "                        depth_te = 1,\n",
        "                        size_bottleneck = 2,\n",
        "                        preprocessing = 'conv',\n",
        "                        im_size = 224,\n",
        "                        patch_size = 16,\n",
        "                        num_channels = 3,\n",
        "                        hidden_dim = 64,\n",
        "                        num_heads = 4,\n",
        "                        attn_drop = 0.2,\n",
        "                        proj_drop = 0.2,\n",
        "                        linear_drop = 0,\n",
        "                        softmax_type = 'standard',\n",
        "                        top_k=150,\n",
        "                        dtype = torch.float32,\n",
        "                        verbose = False\n",
        "                        )\n",
        "        model.to('cuda:0')\n",
        "        criterion = torch.nn.MSELoss(reduction='mean')\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "        # Create fitter\n",
        "        fitter = ImageFitter(model,\n",
        "                                     loss=criterion,\n",
        "                                     optimizer=optimizer,\n",
        "                                     device='cuda',\n",
        "                                     folder='models',\n",
        "                                     use_amp=False)\n",
        "\n",
        "        def wandb_update(x):\n",
        "            data_log = x.copy()\n",
        "            del data_log['epoch']\n",
        "            wandb.log({'training_'+str(fold):data_log})\n",
        "\n",
        "        history = fitter.fit(train_dataloader,\n",
        "                             val_dataloader,\n",
        "                             #metrics=[(peak_signal_noise_ratio, {})]\n",
        "                             #early_stopping=10,\n",
        "                             #early_stopping_mode='min',\n",
        "                             n_epochs=n_epochs,\n",
        "                             callbacks=[wandb_update],\n",
        "                             verbose_steps=1)\n",
        "\n",
        "        fitter.load('models/best-checkpoint.bin')\n",
        "\n",
        "        # Calculate PSNR\n",
        "        model = fitter.model\n",
        "        model.eval()\n",
        "        score = fn.psnr(model, val_dataloader)\n",
        "        print(f\"FOLD {fold}: Mean PSNR {np.mean(score)}\")\n",
        "        results.append(score)\n",
        "        run.log({f'fold_{fold}':{'psnr':np.mean(score)}})\n",
        "\n",
        "        break\n",
        "\n",
        "    print(f\"Average Mean PSNR{np.mean(results)}. STD Mean PSNR {np.std(results)}\")\n",
        "\n",
        "    run.log({'psnr_mean':np.mean(results),\n",
        "             'psnr_std':np.std(results)})\n",
        "\n",
        "    run.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ual/ViT_UNet/runs/tiqdxws6\" target=\"_blank\">exalted-plant-78</a></strong> to <a href=\"https://wandb.ai/ual/ViT_UNet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOLD 0: Training on 1024 samples and testing on 256 samples\n",
            "Architecture information:\n",
            "Level 0:\n",
            "\tPatch size: 16\n",
            "\tNum. patches: 196\n",
            "\tProjection size: 768\n",
            "\tHidden dim. size: 64\n",
            "Level 1:\n",
            "\tPatch size: 8\n",
            "\tNum. patches: 784\n",
            "\tProjection size: 192\n",
            "\tHidden dim. size: 32\n",
            "Level 2:\n",
            "\tPatch size: 4\n",
            "\tNum. patches: 3136\n",
            "\tProjection size: 48\n",
            "\tHidden dim. size: 16\n",
            "Fitter prepared. Device is cuda\n",
            "\n",
            "2021-10-03 11:11:29\n",
            "                         EPOCH 1/1 - LR: 0.0001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Step 18/256 | summary_loss: 2.25507 | time: 73.36 secs | ETA: 918.92"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAZLKOA6KqlE"
      },
      "source": [
        "# Visual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5H3kfU5-Hi6"
      },
      "source": [
        "model.eval()\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        x = batch['x'].to('cuda').float()\n",
        "        output = model(x)\n",
        "        y = batch['y']\n",
        "        results.append((x[0].cpu().numpy(), y[0].numpy(), output[0].cpu().numpy()))\n",
        "\n",
        "original = x[0].cpu().numpy().transpose(1,2,0)\n",
        "clean = y[0].numpy().transpose(1,2,0)\n",
        "reconstructed = output[0].cpu().numpy().transpose(1,2,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvU4yhz0AsCD"
      },
      "source": [
        "for a, b, c in results:\n",
        "    original = a.transpose(1,2,0)\n",
        "    clean = b.transpose(1,2,0)\n",
        "    reconstructed = c.transpose(1,2,0)\n",
        "    fig, ax = plt.subplots(1,3,figsize=(10,10))\n",
        "\n",
        "    ax[0].imshow(original)\n",
        "    ax[1].imshow(clean)\n",
        "    ax[2].imshow(reconstructed)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}